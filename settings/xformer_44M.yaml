model_hyperparams:
  vocab_size: 10_000
  embedding_dim: 384
  sequence_length: 100
  num_layers: 8
  num_heads: 6
  hidden_layer_multiplier: 4 
  dropout: 0.05
  attention: scaled_dot_product
  activation: gelu
  normalization: pre

model_training:
  learning_rate: 0.005
  batch_size: 128
  accumulation_steps: 1
  weight_decay: 0.1
  epochs: 5
  max_epochs: 5
  betas: [0.9 ,0.999]
  warmup_period: 0