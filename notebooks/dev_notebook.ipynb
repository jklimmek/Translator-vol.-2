{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n",
      "Triton is not available, some optimizations will not be enabled.\n",
      "This is just a warning: No module named 'triton'\n",
      "Triton is not available, FusedMLP will not be enabled.\n",
      "Either FairScale or torch distributed is not available, MixtureOfExperts will not be exposed. Please install them if you would like to use MoE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('D:/projects/Torch/Translator vol.2')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sacrebleu.metrics import BLEU\n",
    "from rouge import Rouge\n",
    "from tabulate import tabulate\n",
    "from xformers.factory.model_factory import xFormer, xFormerConfig, xFormerEncoderConfig, xFormerDecoderConfig, xFormerEncoderBlock, xFormerDecoderBlock\n",
    "from tokenizers import Tokenizer\n",
    "from scripts.factory import FactoryModel\n",
    "from scripts.transformer import Transformer\n",
    "from scripts.utils import *\n",
    "from scripts.dataset import *\n",
    "\n",
    "# check gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB = 128\n",
    "SEQ = 100\n",
    "BATCH = 16\n",
    "VOCAB = 64\n",
    "\n",
    "my_config = [\n",
    "    # A list of the encoder or decoder blocks which constitute the Transformer.\n",
    "    # Note that a sequence of different encoder blocks can be used, same for decoders\n",
    "    {\n",
    "        \"reversible\": False,  # Optionally make these layers reversible, to save memory\n",
    "        \"block_type\": \"encoder\",\n",
    "        \"num_layers\": 6,  # Optional, this means that this config will repeat N times\n",
    "        \"dim_model\": EMB,\n",
    "        \"residual_norm_style\": \"pre\",  # Optional, pre/post\n",
    "        \"position_encoding_config\": {\n",
    "            \"name\": \"vocab\",  # whatever position encodinhg makes sense\n",
    "            \"seq_len\": 1024,\n",
    "            \"vocab_size\": VOCAB,\n",
    "        },\n",
    "        \"multi_head_config\": {\n",
    "            \"num_heads\": 4,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"use_rotary_embeddings\": True,\n",
    "            \"attention\": {\n",
    "                \"name\": \"linformer\",  # whatever attention mechanism\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": False,\n",
    "                \"seq_len\": SEQ,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"MLP\",\n",
    "            \"dropout\": 0,\n",
    "            \"activation\": \"relu\",\n",
    "            \"hidden_layer_multiplier\": 4,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"reversible\": False,  # Optionally make these layers reversible, to save memory\n",
    "        \"block_type\": \"decoder\",\n",
    "        \"num_layers\": 6,  # Optional, this means that this config will repeat N times\n",
    "        \"dim_model\": EMB,\n",
    "        \"residual_norm_style\": \"pre\",  # Optional, pre/post\n",
    "        \"position_encoding_config\": {\n",
    "            \"name\": \"vocab\",  # whatever position encodinhg makes sense\n",
    "            \"seq_len\": SEQ,\n",
    "            \"vocab_size\": VOCAB,\n",
    "        },\n",
    "        \"multi_head_config_masked\": {\n",
    "            \"num_heads\": 4,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"use_rotary_embeddings\": True,\n",
    "            \"attention\": {\n",
    "                \"name\": \"linformer\",  # whatever attention mechanism\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": True,\n",
    "                \"seq_len\": SEQ,\n",
    "            },\n",
    "        },\n",
    "        \"multi_head_config_cross\": {\n",
    "            \"num_heads\": 4,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"linformer\",  # whatever attention mechanism\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": True,\n",
    "                \"seq_len\": SEQ,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"MLP\",\n",
    "            \"dropout\": 0,\n",
    "            \"activation\": \"relu\",\n",
    "            \"hidden_layer_multiplier\": 4,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "# This part of xFormers is entirely type checked and needs a config object,\n",
    "# could be changed in the future\n",
    "config = xFormerConfig(my_config)\n",
    "model = xFormer.from_config(config)\n",
    "out = torch.nn.Linear(EMB, VOCAB)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay = []\n",
    "no_decay = []\n",
    "no_decay_layers = [\"norm\", \"bias\"]\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if any(nd in name for nd in no_decay_layers):\n",
    "        no_decay.append(name)\n",
    "    else:\n",
    "        decay.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 156)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decay), len(no_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EMB = 128\n",
    "# # SEQ = 100\n",
    "# # BATCH = 16\n",
    "# # VOCAB = 64\n",
    "# x = torch.randint(0, VOCAB, (1, SEQ)).to(torch.int64)\n",
    "# y = torch.randint(0, VOCAB, (1, SEQ + 1)).to(torch.int64)\n",
    "# y_src, y_tgt = y[:, :-1], y[:, 1:]\n",
    "# # o: (batch, seq, EMB)\n",
    "# o = model(src=x, tgt=torch.tensor([[1]]).to(torch.int64))\n",
    "# # o: (batch, seq, VOCAB)\n",
    "# o = out(o)\n",
    "# loss = loss_fn(o.permute(0,2,1), y_tgt)\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2771180, 593464)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparams = yaml_to_kwargs(\"../settings/classic_hyperparams.yaml\")\n",
    "# model = Translator(**hyperparams[\"model_hyperparams\"], **hyperparams[\"model_training\"])\n",
    "\n",
    "tr = torch.load(\"../data/datasets/train-10000.pt\")\n",
    "dv = torch.load(\"../data/datasets/dev-10000.pt\")\n",
    "len(tr), len(dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting tokens: 100%|█████████████████████████████████| 2771180/2771180 [02:35<00:00, 17831.47it/s]\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(ds):\n",
    "    src_tokens = 0\n",
    "    tgt_tokens = 0\n",
    "    for src, tgt in tqdm(ds, total=len(ds), desc=\"Counting tokens\", ncols=100):\n",
    "        src_tokens += torch.sum(src != 0).item()\n",
    "        tgt_tokens += torch.sum((tgt != 0) & (tgt != 2) & (tgt != 3)).item()\n",
    "    return src_tokens, tgt_tokens\n",
    "src_tokens, tgt_tokens = count_tokens(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained on: 88 082 172 German tokens\n",
      "Model trained on: 83 294 345 English tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"Model trained on: {:,} German tokens\".format(src_tokens).replace(\",\", \" \"))\n",
    "print(\"Model trained on: {:,} English tokens\".format(tgt_tokens).replace(\",\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_tok = Tokenizer.from_file(\"../tokenizers/de_tokenizer_10000.json\")\n",
    "en_tok = Tokenizer.from_file(\"../tokenizers/en_tokenizer_10000.json\")\n",
    "de = read_file(\"../data/train/train.de\")\n",
    "en = read_file(\"../data/train/train.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_token_word_ratio(de, en, de_tok, en_tok):\n",
    "    for src, tgt in zip(de, en):#tqdm(zip(de, en), total=len(de), desc=\"Counting tokens\", ncols=100):\n",
    "        src_tokens = len(de_tok.encode(src).tokens)\n",
    "        tgt_tokens = len(en_tok.encode(tgt).tokens)\n",
    "        print(de)\n",
    "        print(src_tokens, tgt_tokens)\n",
    "        break\n",
    "        src_words = len(src.split())\n",
    "        tgt_words = len(tgt.split())\n",
    "\n",
    "de_ratio, en_ratio = calculate_token_word_ratio(de, en, de_tok, en_tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|███████████████████████████████████████| 3128188/3128188 [06:56<00:00, 7514.61it/s]\n"
     ]
    }
   ],
   "source": [
    "min_len = 10\n",
    "max_len = 100\n",
    "unk_percentage = 0.15\n",
    "length_tolerance = 2\n",
    "de_hash_count = 0\n",
    "en_hash_count = 0\n",
    "de_count = 0\n",
    "en_count = 0\n",
    "\n",
    "for de_line, en_line in tqdm(zip(de, en), total=len(de), desc=\"Tokenizing\", ncols=100):\n",
    "        de_tokens = de_tok.encode(de_line).tokens\n",
    "        en_tokens = en_tok.encode(en_line).tokens\n",
    "        if (\n",
    "            min_len >= len(de_tokens)\n",
    "            or len(de_tokens) >= max_len\n",
    "            or min_len >= len(en_tokens)\n",
    "            or len(en_tokens) >= max_len\n",
    "        ):\n",
    "            continue\n",
    "        if (\n",
    "            de_tokens.count(\"[UNK]\") / len(de_tokens) > unk_percentage\n",
    "            or en_tokens.count(\"[UNK]\") / len(en_tokens) > unk_percentage\n",
    "        ):\n",
    "            continue\n",
    "        if max(len(de_tokens), len(en_tokens)) / min(len(de_tokens), len(en_tokens)) > length_tolerance:\n",
    "            continue\n",
    "        \n",
    "        en_tokens = en_tokens[1:-1]\n",
    "\n",
    "        de_hash_count += sum(1 for token in de_tokens if \"##\" in token)\n",
    "        en_hash_count += sum(1 for token in en_tokens if \"##\" in token)\n",
    "        de_count += len(de_tokens)\n",
    "        en_count += len(en_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German word:token ratio 0.71\n",
      "English word:token ratio 0.90\n"
     ]
    }
   ],
   "source": [
    "# words / tokens\n",
    "print(\"German word:token ratio {:.2f}\".format((de_count - de_hash_count) / de_count))\n",
    "print(\"English word:token ratio {:.2f}\".format((en_count - en_hash_count) / en_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\anaconda3\\envs\\torch_gpu\\lib\\site-packages\\pytorch_lightning\\utilities\\migration\\utils.py:49: PossibleUserWarning: The loaded checkpoint was produced with Lightning v2.0.3, which is newer than your current Lightning version: v2.0.1\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "de_tok = Tokenizer.from_file(\"../tokenizers/de_tokenizer_10000.json\")\n",
    "en_tok = Tokenizer.from_file(\"../tokenizers/en_tokenizer_10000.json\")\n",
    "models = \"../inference_models/\"\n",
    "factory_checkpoint = \"epoch=04-train_loss=0.6199-val_loss=0.5762_fct_44m.ckpt\"\n",
    "factory_model = FactoryModel.load_from_checkpoint(models + factory_checkpoint)\n",
    "factory_model.eval();\n",
    "\n",
    "checkpoint = \"epoch=04-train_loss=0.6483-val_loss=0.5983_trs_18m.ckpt\"\n",
    "params = \"../settings/transformer_18M.yaml\"\n",
    "model = load_model(models + checkpoint, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i am a berliner.',\n",
       " [0.9458821415901184,\n",
       "  0.8245789408683777,\n",
       "  0.9143891334533691,\n",
       "  0.9696773290634155,\n",
       "  0.972100019454956,\n",
       "  0.9485659599304199,\n",
       "  0.9833275675773621])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def factory_translate(sentence, model, de_tok, en_tok, maxlen=100, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    de_tok.enable_padding(length=maxlen)\n",
    "    x = torch.tensor(de_tok.encode(sentence, add_special_tokens=False).ids, device=device).unsqueeze(0)\n",
    "    y = torch.zeros((1, maxlen), dtype=torch.long, device=device)\n",
    "    y[0, 0] = en_tok.token_to_id(\"[SOS]\")\n",
    "    for i in range(1, maxlen):\n",
    "        logits = model(src=x, tgt=y)\n",
    "        token = logits[0, i-1].topk(1)[1].item()\n",
    "        y[0, i] = token\n",
    "        if token == en_tok.token_to_id(\"[EOS]\"):\n",
    "            break\n",
    "    return en_tok.decode(y.tolist()[0])\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate(sentence, model, de_tok, en_tok, maxlen=100, return_probs=False, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Translates a sentence from a source language to a target language using a translation model.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence to be translated.\n",
    "        model (torch.nn.Module): The translation model.\n",
    "        de_tok (Tokenizer): The tokenizer for the source language.\n",
    "        en_tok (Tokenizer): The tokenizer for the target language.\n",
    "        maxlen (int, optional): The maximum length of the translated sentence. Defaults to 100.\n",
    "        return_probs (bool, optional): Whether to return the probabilities of the tokens. Defaults to False.\n",
    "        device (str, optional): The device to run the translation on. Defaults to \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        str: The translated sentence.\n",
    "        list: The probabilities of the tokens.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    de_tok.enable_padding(length=maxlen)\n",
    "    de_tok.enable_truncation(max_length=maxlen)\n",
    "    x = torch.tensor(de_tok.encode(sentence, add_special_tokens=False).ids, device=device) \n",
    "    y = torch.tensor([[en_tok.token_to_id(\"[SOS]\")]], dtype=torch.long, device=device)\n",
    "    probs = []\n",
    "    while y.size(1) < maxlen:\n",
    "        tgt_mask = model._generate_square_subsequent_mask(y.size(1)).to(device)\n",
    "        logits = model(x, y, tgt_mask=tgt_mask)\n",
    "        prob, token = F.softmax(logits, dim=-1).topk(1)\n",
    "        probs.append(prob[-1].item())\n",
    "        token = torch.tensor([[token[-1].item()]], device=device)\n",
    "        y = torch.cat((y, token), dim=1)\n",
    "        if token == en_tok.token_to_id(\"[EOS]\"):\n",
    "            break\n",
    "    translated = en_tok.decode(y.tolist()[0])\n",
    "    if return_probs is True:\n",
    "        return translated, probs\n",
    "    return translated\n",
    "\n",
    "translate(\"Ich bin ein Berliner.\", model, de_tok, en_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"natürlich kann das kein argument seidenn wenn etwas passiersind große flächen verseucht .\"\n",
    "s2 = \"2 und zur gleichen zeit hast du auch deine gabe verloreund dein a verstand hat sich verfinstert .\"\n",
    "s3 = \"ich bitte die kommission um ihre stellungnahme zu den änderungsanträgen .\"\n",
    "s4 = \"um welchen artikel der geschäftsordnung geht es ?\"\n",
    "s5 = \"ich möchte sie bitten sich zu setzen .\"\n",
    "s6 = \"zu einer kühnen konstruktion aus stahl und glas kontrastieren warme holzpanelen .\"\n",
    "s7 = \"wir zielen mit unserer auffassung zur ständigen verbesserung durch anwendung von innovationen und technologien in unser system das beste für unsere kunden auso daß wir durch deckung aller technischen und technologischen bedürfnisse eine starke corporate identity beweisen .\"\n",
    "s8 = \"viele fragen sind wirklich nervig aber keine ist besonders dumm .\"\n",
    "s9 = \"Natürlich ist uns bewußdaß im Rahmen der Typprüfung 70/156, dass bisher keine Maßstäbe für die Erfassung der CO2-Emission für leichte Nutzfahrzeuge vorhanden sind.\"\n",
    "s10 = \"auch das liegt leider auf einer linie mit den abstimmungen der letzten wocheund wir bedauern das außerordentlich .\"\n",
    "s11 = \"Bitte sprechen Sie während der Fahrt nicht mit dem Fahrer.\"\n",
    "s12 = \"Ich habe die Ehre, Sie zu einem Glas Wein einzuladen.\"\n",
    "s13 = \"Ich bin nicht sicher, ob ich das richtig verstanden habe.\"\n",
    "s14 = \"Die Sonne scheint und Kinder spielen draußen.\"\n",
    "s15 = \"im september wird die kommission ihre vorschläge für indikatoren vorlegeanhand derer wir ermitteln könnewie gut wir bei der erfüllung der in lissabon gesetzten ziele vorangekommen sind .\"\n",
    "s16 = \"die idedie hinter der entwicklung des aulochrome am 10.09.2001 und dessen fertigstellung im jahre 2002 steckisdass ein saxophon nur einen ton gleichzeitig spielen kann .\"\n",
    "sentences = [\n",
    "    \"Obwohl es geregnet hat, sind wir trotzdem zum Strand gegangen.\",\n",
    "    \"Die Veranstaltung war ein großer Erfolg, dank der hervorragenden Organisation.\",\n",
    "    \"Ich habe meine Deutschkenntnisse verbessert, indem ich regelmäßig Bücher gelesen habe.\",\n",
    "    \"Es ist wichtig, eine gesunde Work-Life-Balance zu haben, um Stress zu reduzieren.\",\n",
    "    \"Mein Bruder hat sich für ein Stipendium beworben, damit er sein Studium finanzieren kann.\",\n",
    "    \"Je mehr ich lerne, desto selbstbewusster werde ich in der Sprache.\",\n",
    "    \"Der Film war so fesselnd, dass ich bis spät in die Nacht aufgeblieben bin, um ihn zu Ende zu sehen.\",\n",
    "    \"Das Buch, das ich gerade lese, handelt von einer abenteuerlichen Reise um die Welt.\", #,\n",
    "    \"Nachdem wir das Konzert besucht hatten, trafen wir uns mit Freunden zum Abendessen.\", #,\n",
    "    \"Ich würde gerne eine Fremdsprache fließend sprechen können, um meine beruflichen Möglichkeiten zu erweitern.\"\n",
    "]\n",
    "\n",
    "translations = [\n",
    "    \"Although it was raining, we still went to the beach.\",\n",
    "    \"The event was a great success, thanks to the excellent organization.\",\n",
    "    \"I improved my German skills by regularly reading books.\",\n",
    "    \"It is important to have a healthy work-life balance in order to reduce stress.\",\n",
    "    \"My brother applied for a scholarship in order to finance his studies.\",\n",
    "    \"The more I learn, the more confident I become in the language.\",\n",
    "    \"The movie was so captivating that I stayed up late into the night to finish watching it.\",\n",
    "    \"The book I'm currently reading is about an adventurous journey around the world.\",\n",
    "    \"After attending the concert, we met up with friends for dinner.\",\n",
    "    \"I would love to be able to speak a foreign language fluently in order to broaden my career opportunities.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'after visiting the concert we met friends with dinner.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factory_translate(\"Nachdem wir das Konzert besucht hatten trafen wir uns mit Freunden zum Abendessen.\", factory_model, de_tok, en_tok, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obwohl es geregnet hat, sind wir trotzdem zum Strand gegangen.\n",
      "although it has been cleaned, we have gone to the beach.\n",
      "Although it was raining, we still went to the beach. \n",
      "\n",
      "Die Veranstaltung war ein großer Erfolg, dank der hervorragenden Organisation.\n",
      "the event was a great success, thanks to the excellent organization.\n",
      "The event was a great success, thanks to the excellent organization. \n",
      "\n",
      "Ich habe meine Deutschkenntnisse verbessert, indem ich regelmäßig Bücher gelesen habe.\n",
      "i have improved my german knowledge, and i have read books regularly.\n",
      "I improved my German skills by regularly reading books. \n",
      "\n",
      "Es ist wichtig, eine gesunde Work-Life-Balance zu haben, um Stress zu reduzieren.\n",
      "it is important to have a healthy work life balance, in order to reduce stress.\n",
      "It is important to have a healthy work-life balance in order to reduce stress. \n",
      "\n",
      "Mein Bruder hat sich für ein Stipendium beworben, damit er sein Studium finanzieren kann.\n",
      "my brother has been standing up for a scholarship, which will allow him to finance his studies.\n",
      "My brother applied for a scholarship in order to finance his studies. \n",
      "\n",
      "Je mehr ich lerne, desto selbstbewusster werde ich in der Sprache.\n",
      "the more i learn, the more self confident i am in the language.\n",
      "The more I learn, the more confident I become in the language. \n",
      "\n",
      "Der Film war so fesselnd, dass ich bis spät in die Nacht aufgeblieben bin, um ihn zu Ende zu sehen.\n",
      "the film was so fascinating, so that i was able to be recorded in the night until late, to see him at the end.\n",
      "The movie was so captivating that I stayed up late into the night to finish watching it. \n",
      "\n",
      "Das Buch, das ich gerade lese, handelt von einer abenteuerlichen Reise um die Welt.\n",
      "the book i read is a journey of adventure around the world.\n",
      "The book I'm currently reading is about an adventurous journey around the world. \n",
      "\n",
      "Nachdem wir das Konzert besucht hatten, trafen wir uns mit Freunden zum Abendessen.\n",
      "after visiting the concert we met friends with dinner.\n",
      "After attending the concert, we met up with friends for dinner. \n",
      "\n",
      "Ich würde gerne eine Fremdsprache fließend sprechen können, um meine beruflichen Möglichkeiten zu erweitern.\n",
      "i would like to speak a foreign language fluently, in order to broaden my professional opportunities.\n",
      "I would love to be able to speak a foreign language fluently in order to broaden my career opportunities. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s, t in zip(sentences, translations):\n",
    "    print(s)\n",
    "    print(factory_translate(s, factory_model, de_tok, en_tok, maxlen=100))\n",
    "    print(t, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = [token for token in de_tok.encode(s1, add_special_tokens=False).tokens if token != \"[PAD]\"]\n",
    "# merged = [tokens[0]]\n",
    "# for token in tokens:\n",
    "#     if \"##\" in token:\n",
    "#         merged[-1] += token[2:]\n",
    "#     else:\n",
    "#         merged.append(token)\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([  14, 1283,  210,  105,  432, 1242,  274,  157,   96, 2957, 1006,  434,\n",
       "         568, 8079,  250, 9952,  630,  439,  913, 1216, 4849,  228,  533,  121,\n",
       "        3579, 1254,  144, 2801, 1911, 6977,   58, 7527,   10,  964,  218, 2107,\n",
       "        1386,  177, 9705, 6954,  670,   26, 8277, 1226,   10,  167,  312,  195,\n",
       "        4730, 3244,  714,   10]), batch_sizes=tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1]), sorted_indices=tensor([1, 0, 2]), unsorted_indices=tensor([1, 0, 2]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [de_tok.encode(s, add_special_tokens=False).ids for s in [s1, s2, s3]]\n",
    "packed = pack_sequence([torch.tensor(t) for t in tokens], enforce_sorted=False)\n",
    "packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1283,  432,  157, 1006, 8079,  630, 1216,  533, 1254, 1911, 7527,  218,\n",
       "          1386, 9705,  670, 8277,   10,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [  14,  105,  274, 2957,  568, 9952,  913,  228, 3579, 2801,   58,  964,\n",
       "          2107,  177, 6954,   26, 1226,  167,  312,  195, 4730, 3244,  714,   10],\n",
       "         [ 210, 1242,   96,  434,  250,  439, 4849,  121,  144, 6977,   10,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       " tensor([17, 24, 11]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_packed_sequence(packed, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m translate(\u001b[39m\"\u001b[39;49m\u001b[39mBitte sprechen Sie während der Fahrt nicht mit dem Fahrer.\u001b[39;49m\u001b[39m\"\u001b[39;49m, model, de_tok, en_tok)\u001b[39m.\u001b[39mcapitalize()\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36mtranslate\u001b[1;34m(sentence, model, de_tok, en_tok, maxlen, device)\u001b[0m\n\u001b[0;32m      6\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[en_tok\u001b[39m.\u001b[39mtoken_to_id(\u001b[39m\"\u001b[39m\u001b[39m[SOS]\u001b[39m\u001b[39m\"\u001b[39m)]], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m      7\u001b[0m \u001b[39mwhile\u001b[39;00m y\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39m<\u001b[39m maxlen:\n\u001b[0;32m      8\u001b[0m     \u001b[39m# tgt_mask = model._generate_square_subsequent_mask(y.size()[-1]).to(device)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[39m# logits = model(x, y, tgt_mask=tgt_mask)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     logits \u001b[39m=\u001b[39m model(src\u001b[39m=\u001b[39;49mx, tgt\u001b[39m=\u001b[39;49my)\n\u001b[0;32m     11\u001b[0m     token \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mtopk(\u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mitem() \n\u001b[0;32m     12\u001b[0m     token \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[token]], device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\projects/Torch/Translator vol.2\\scripts\\factory.py:122\u001b[0m, in \u001b[0;36mFactoryModel.forward\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src, tgt):\n\u001b[1;32m--> 122\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxformer(src, tgt)\n\u001b[0;32m    123\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(x)\n\u001b[0;32m    124\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(x)\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\torch_gpu\\lib\\site-packages\\xformers\\factory\\model_factory.py:276\u001b[0m, in \u001b[0;36mxFormer.forward\u001b[1;34m(self, src, tgt, encoder_input_mask, decoder_input_mask)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(encoders, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModuleList):\n\u001b[0;32m    275\u001b[0m     \u001b[39mfor\u001b[39;00m encoder \u001b[39min\u001b[39;00m encoders:\n\u001b[1;32m--> 276\u001b[0m         memory \u001b[39m=\u001b[39m encoder(memory, input_mask\u001b[39m=\u001b[39;49mencoder_input_mask)\n\u001b[0;32m    277\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrev_enc_pose_encoding:\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\torch_gpu\\lib\\site-packages\\xformers\\factory\\block_factory.py:217\u001b[0m, in \u001b[0;36mxFormerEncoderBlock.forward\u001b[1;34m(self, x, att_mask, input_mask)\u001b[0m\n\u001b[0;32m    214\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_emb(x)\n\u001b[0;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpose_encoding \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 217\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpose_encoding(x)\n\u001b[0;32m    219\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39membedding_projector\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    220\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_projector(x)\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\torch_gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\MSI\\anaconda3\\envs\\torch_gpu\\lib\\site-packages\\xformers\\components\\positional_embedding\\vocab.py:55\u001b[0m, in \u001b[0;36mVocabEmbedding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m---> 55\u001b[0m     position_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(x\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice)[\n\u001b[0;32m     56\u001b[0m         \u001b[39mNone\u001b[39;00m, :\n\u001b[0;32m     57\u001b[0m     ]\u001b[39m.\u001b[39mrepeat(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m)\n\u001b[0;32m     59\u001b[0m     X_token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embeddings(x)\n\u001b[0;32m     60\u001b[0m     X_pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings(position_ids)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "translate(s1, model, de_tok, en_tok).capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 103 740 416\n"
     ]
    }
   ],
   "source": [
    "folder = \"../runs/vs-10000__act-gelu__n_layers-6__dp-0.1/betas-[0.9, 0.999]__wd-0.1__bs-100__max_ep-5__acc-1/\"\n",
    "checkpoint = \"epoch=04-train_loss=0.6483-val_loss=0.5983.ckpt\"\n",
    "params = yaml_to_kwargs(\"../settings/hyperparams.yaml\")\n",
    "# model = Transformer.load_from_checkpoint(folder + checkpoint, **params[\"model_hyperparams\"], **params[\"model_training\"])\n",
    "# model = Transformer(**params[\"model_hyperparams\"], **params[\"model_training\"])\n",
    "model = FactoryModel(**params[\"model_hyperparams\"], **params[\"model_training\"])\n",
    "\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Model params: {:,}\".format(params).replace(\",\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
